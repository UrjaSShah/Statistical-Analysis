# -*- coding: utf-8 -*-
"""PHYS-8823-HW-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ijiitTmTdi0H1iTEtZTwiDr7foK7z9vQ

# PHYS 8823: Homework 2
"""

# Commented out IPython magic to ensure Python compatibility.
# Imports
# %matplotlib inline
import numpy as np
import scipy.stats as st
import matplotlib.pyplot as plt

"""### Problem 1
#### Part 1
We want to know what the probability is of a randomly selected ELT is not damaged, i.e. $P(nD)=1- P(D)$, where $P(nD)$ is the probability of a non-damaged ELT and $P(D)$ is the probability of a defected ELT. Letting $P(D|i)$ be the probability of getting a damaged ELT given company $i$ ($A, B, C$ represent AM Corporation, Bryant Company, and Chartair ELT's respectively), we can write the following equation:
\begin{equation}
    \begin{aligned}
    P(nD) &= 1 - P(D)\\
    &= 1 - (P(D|A)P(A) + P(D|B)P(B) + P(D|C)P(C))\\
    \end{aligned}
\end{equation}

#### Part 2
We want to know, given that the ELT is damaged, what the probability is that it was made by company $i$: $P(i|D)$. This can be solved using Bayes' Theorem:
\begin{equation}
    P(i|D) = \frac{P(D|i)P(i)}{P(D)}
\end{equation}
"""

# 1 #

# Constants
PA = 0.80          # Prob. ELT made by AM Corp.
PB = 0.15          # Prob. ELT made by Bryant Co.
PC = 0.05          # Prob. ELT made by Chartair
A_def = 0.04       # Prob. AM Corp ELT defective
B_def = 0.06       # Prob. Braynt Co ELT defective
C_def = 0.09       # Prob. Chartair ELT defective

# Calculate Probability that randomly selected ELT is NOT damaged
P_nD = 1 - (PA*A_def + PB*B_def + PC*C_def)
print('The probability that a randomly selected ELT is not damaged is ', P_nD,'.')

# Defining Bayes Theorem for this problem
def BayesELT(Pi, i_def, P_D):
    return ((i_def*Pi)/P_D)

P_D = PA*A_def + PB*B_def + PC*C_def

print('Given that a randomly selected ELT is damaged, the probability that it was made by AM Corp. is ', \
      '%.4f' % BayesELT(PA,A_def,P_D),'.')
print('Given that a randomly selected ELT is damaged, the probability that it was made by Bryant Co. is ', \
      '%.4f' % BayesELT(PB,B_def,P_D),'.')
print('Given that a randomly selected ELT is damaged, the probability that it was made by Chartair is ', \
      '%.4f' % BayesELT(PC,C_def,P_D),'.')

"""### Problem 2
We can use the Poisson limit because of the large number of atoms $N$ in one nanogram of Pu-239. A rough calculation shows that there are approximately $2\times10^{-12}$ atoms in $1$ ng of Pu-239.
\begin{equation*}
    \begin{aligned}
        \frac{4\times10^{-12}\text{ mol}}{1 \text{ ng}}
        \frac{6\times10^{23} \text{ atoms}}{1 \text{ mol}} \approx 2 \times 10^{12} \text{ atoms per nanogram}
    \end{aligned}
\end{equation*}
The Poisson limit for a large $N$ Binomial distribution is
\begin{equation}
    P_{pois}(n;\lambda) = \frac{\lambda^n}{n!}e^{-\lambda}
\end{equation}
where $n$ is the number of successes (atom decays) and $\lambda$, in this case, is the finite constant $\lambda = \mu t$ ($\mu$ is the average per unit time). Since we are interested in a $2$ second period, $\lambda = \mu\times2 =2.3\times 2=4.6$.
To do the probability that there are 3 or more decays, i.e. the sum of probabilities for $n=3\to\inf$. Alternatively, we know that the total probability is $1$ and we don't care about the probability that $n=0,1,2$. Thus,
\begin{equation}
    \begin{aligned}
        P(n\geq3) &= 1- P(0) - P(1) - P(2)\\
    \end{aligned}
\end{equation}
"""

# 2 #

# Constants
mu = 2.3       # Average decay per unit time (s)
t = 2          # Time period (s)
l = mu*t       # Average decay in t units

# Defining Poisson distribution for some n and lambda
def Pois(n,l):
    return(((l**n)/np.math.factorial(n))*np.exp(-l))

# Poisson distribution, n=3
print('The probability that in a 2 second period there are exactly 3 decays is', \
     '%.4f' % Pois(3,l),'.')

# Poisson distribution for n=>3 (1-P(0)-P(1)-P(2))
print('The probability that in a 2 second period there are 3 or more decays is', \
     '%.4f' % (1- Pois(0,l)-Pois(1,l)-Pois(2,l)),'.')

"""### Problem 3
I am choosing to describe this situation with a binomial distribution where the total number of events $N$ is 16, the number of "successes" (alert on a weekend) $n$ is 11, and the probability $p$ that an alert occurs on a weekend is $n/N = 0.6875$. I am using the Binomial distribution versus, say a Poisson distribution, because I don't believe that $N$ is sufficiently large enough to justify that approximation. Additionally, it can be described by a Binomial distribution because there are $N$ independent Bernoulli processes (binary outcome: alert on a weekend, versus alert not on a weekend). Taking this $p$ to be the null hypothesis, I will use the SciPy binomial test as it "is a test of the null hypothesis that the probability of success in a Bernoulli experiment $p$" and returns the $p-value$.
"""

# 3 #

# Constants
n = 11          # Number of "successes" (alert on a weekend)
N = 16          # Number of total events
p = n/N         # Given probability of alert on a weekend (null hypothesis)

# P-Value Calculation, Binomial Distribution
binom = st.binomtest(n, N, p, alternative='greater')
p_val = binom.pvalue


print('The probability that 11 or more alerts out of 16 fall on a weekend is', \
     '%.4f' % p_val,'.')

"""### Problem 4"""

# 4 #

# Constants
Np = 0.5       # <n> for Binomial distribution
lam = 0.5      # Lambda from Poisson distribution

# Histogram binning (goes from 0-10, bins centered on integer)
bins = np.arange(-0.5,11.5,1.)

# Loop to test various Ns for Binomial distribution - limit where Binomial reaches Poisson
for N in [1.,10.,100.]:
    #Poisson distribution
    pois = np.random.poisson(lam, 1000000)

    # Binomial distribution
    p = Np/N
    binom = np.random.binomial(N, p, 1000000)

    # Plot Binomial and Poisson distributions together
    BinomialLabel = "Binomial, N = " + str(int(N))
    plt.hist(pois,bins,density=True, histtype=u'step',lw=2,label="Poisson")
    plt.hist(binom,bins,density=True,label=BinomialLabel,histtype=u'step',lw=2)
    plt.legend()
    plt.xlabel('n')
    plt.ylabel('Distribution')
    plt.title("Binomial vs Poisson, $N=$" + str(int(N)))
    plt.show()

"""### Problem 5"""

# 5 #

# Loop to test various Lambdas for Binomial distribution - limit where Binomial reaches Poisson
for lam in [1.0,10.,100.]:
    #Poisson distribution
    pois = np.random.poisson(lam, 1000000)

    #Normal Distribution
    norm = np.random.normal(lam,np.sqrt(lam), 1000000) #set mu=lam and sigma=sqrt(lam) so plots are centered the same

    #Plot
    NormLabel = "Gaussian, $\mu=$" + str(lam)
    PoisLabel = "Poisson, $\lambda$ =" +str(lam)
    plt.hist(pois,bins=50,density=True, histtype=u'step',lw=2,label=PoisLabel)
    plt.hist(norm,bins=50,density=True, histtype=u'step',lw=2,label=NormLabel)
    plt.xlabel('x')
    plt.ylabel('Distribution')
    plt.title("Poisson vs Gaussian, $\lambda=$" + str(int(lam)))
    plt.legend()
    plt.show()

"""### Problem 6"""

# 6 #

# Part A #
print("Part A:\n")

# Triangular distribution between [-1,1], peaks at 0 (1000 random numbers)
tri = np.random.triangular(-1, 0, 1, 1000)

# Plotting
plt.hist(tri, bins=50, density = True, label="Random")
plt.plot([-1,0,1], [0,1,0], label="Expected") #Plotting the triangular distribution (because density=true, normalized to peak at 1)
plt.title("Triangular Distribution")
plt.xlabel('x')
plt.ylabel('Distribution')
plt.legend()
plt.show()


# Part B #

print("Part B:\n")

# Array Initialization
num = [1, 2, 5, 10]    # number of samples
#num = [1, 20, 100, 1000]
means = []             # array of means

# Loop to average a list of 1,2,5,10 numbers sampled from triangular distribution
for j in num:
    x = [np.mean(np.random.choice(tri,j)) for _i in range(1000)]
    means.append(x)

# Plotting
k = 0
fig, ax = plt.subplots(2, 2, figsize =(10, 10))
for i in range(0, 2):
    for j in range(0, 2):
        # Plot histogram for each x stored in means
        ax[i, j].hist(means[k], 10, density = True)
        ax[i, j].set_title(label = '$n=$' + str(num[k]))

        # Normal Distribution
        mu, std = st.norm.fit(means[k]) # mean and standard deviation

        # Plot the PDF.
        xmin, xmax = ax[i, j].get_xlim()
        x = np.linspace(xmin, xmax, 100)
        p = st.norm.pdf(x, mu, std)
        ax[i, j].plot(x, p, linewidth=2,label='Gaussian')
        ax[i, j].set_xlabel('x')
        ax[i, j].set_ylabel('Distribution')
        ax[i, j].legend()
        k = k + 1
plt.show()


# Part C #

print("Part C:\n")

# Average
average = np.average(tri)
print('The average is', '%.4f' % average,'.')

# Unbiased variance
variance = np.var(tri,ddof=1)
print('The unbiased variance is', '%.4f' % variance,'.')

# Standard deviation
sigma = np.sqrt(variance)
print('The standard deviation is ', '%.4f' % sigma,'.')

# Skewness
skew = st.skew(tri)
print('The skewness is ', '%.4f' % skew,'.')

# Kurtosis
kurtosis = st.kurtosis(tri, fisher=True)
print('The kurtosis is ', '%.4f' % kurtosis,'.')

"""### Hours spent on homework: 5.5"""

