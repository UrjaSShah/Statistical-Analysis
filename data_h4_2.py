# -*- coding: utf-8 -*-
"""data_H4-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qbg12RZT8IzZWhuJUzvVofspqCxUrk4M

# 1. Maximum Likelihood.

## 1.
"""

import numpy as np
np.random.seed(7)
dataset = np.random.randint(0, 9+1)

print("To run this notebook, please first download 'Project4_Set{}.txt' to the directory same with this .ipynb.".format(dataset))
E = np.loadtxt("Project4_Set{}.txt".format(dataset))
# print(E)

"""## 2."""

N = E.size
print("The total number of photons, N, is", N)

"""## 3."""

import matplotlib.pyplot as plt

fig, ax = plt.subplots(1,2, figsize = (16,6))

ax[0].hist(E, density = True, histtype = "step", bins = 100)
ax[0].set_title("the distribution of photon energies")
ax[0].set_xlabel("$E\ [\mathrm{a.u.}]$")
ax[0].set_ylabel("PDF [a.u.$^{-1}$]")

ax[1].hist(E, density = True, histtype = "step", bins = 100)
ax[1].set_title("the distribution of photon energies")
ax[1].set_xscale("log")
ax[1].set_xlabel("$E\ [\mathrm{a.u.}]$")
ax[1].set_ylabel("PDF [a.u.$^{-1}$]")

plt.show()

"""It's clear that the PDF reaches local maximum around $E \sim 200$ a.u.. Since the maximum of exponential distribution $f_B$ is reached at $E = 0$ a.u., we infer that $f_s(E)$ reaches local maximum at $E_0 \sim 200$ a.u..

## 4.

The likelihood function is
\begin{align*}
L(\vec E; n_s, E_0, \Gamma) &= \prod_{i=1}^N f(E_i; n_s, E_0, \Gamma)
\end{align*}
where $f(E_i; n_s, E_0, \Gamma)$ is the probability to detect a photon with energy $E_i$. Hence,
\begin{align*}
f(E_i; n_s, E_0, \Gamma) &= \frac{n_s}{N}f_s(E_i) + \frac{n_b}{N}f_B(E_i)\\
&= \frac{n_s}{N}\frac{1}{\pi \Gamma \left[1+ \left(\frac{E_i - E_0}{\Gamma}\right)^2\right]} + \frac{N - n_s}{N}\frac{1}{E_b}\exp(-E_i/E_b)%\\
%&= \frac{1}{E_b}\exp(-E_i/E_b) + \frac{n_s}{N}\left\{\frac{1}{\pi \Gamma \left[1+ \left(\frac{E_i - E_0}{\Gamma}\right)^2\right]} - \frac{1}{E_b}\exp(-E_i/E_b)\right\}
\end{align*}
Maximizing $L$ is equivalent to minimizing
\begin{align*}
-2\log(L) &= -2\sum_{i=1}^N \log f(E_i; n_s, E_0, \Gamma)
\end{align*}
"""

from scipy.optimize import minimize

E_b = 100

def f_s(E):
    return 1/(np.pi * Gamma * (1 + ((E - E_0)/Gamma)**2))#/(1/2+1/np.pi/np.arctan(E_0/Gamma))

def f_B(E):
    return np.e**(-E/E_b)/E_b

def f_both(E):
    return n_s/N * f_s(E) + (N - n_s)/N * f_B(E)

def loglikelihood(arguments):
    global n_s, E_0, Gamma
    n_s, E_0, Gamma = arguments
    return -2*np.sum(np.log(f_both(E)))

initial_guess = [N/2, 200, 100]
bounds = ((0,N), (0, 500), (1, 200))
min_outcome = minimize(loglikelihood, x0 = initial_guess, bounds = bounds)

n_s, E_0, Gamma = min_outcome.x
n_b = N - n_s

logL_min = min_outcome.fun
E_0_fit, Gamma_fit = E_0, Gamma
# backup the fit value because I will change them later

print("The minimization was successful:", min_outcome.success)
print("The minimum value of -2logL is :", logL_min)
print("Best fit values are \nn_s = {:.0f}, \nn_b = {:.0f}, \nE_0 = {:.5f} a.u., \nGamma = {:.5f} a.u..".format(n_s, n_b, E_0, Gamma))

"""## 5."""

E_max = 400
E_array = np.linspace(0, E_max, 100)

fig, ax = plt.subplots(1,1, figsize = (10,6))
counts, bins, patches = ax.hist(E, density = True, histtype = "step", bins = 25, label = "histogram", range = (0,400))
ax.plot(E_array, f_s(E_array), label = 'signal')
ax.plot(E_array, f_B(E_array), label = 'background')
ax.plot(E_array, f_both(E_array), label = 'signal+background')
# err = np.abs(counts - f_both(bins[0:-1]))

ax.set_title("the distribution of photon energies")
ax.set_xlabel("$E\ [\mathrm{a.u.}]$")
ax.set_ylabel("PDF [a.u.$^{-1}$]")
ax.set_xlim(0,E_max)
plt.legend()
plt.show()

"""## 6.

"""

def logL(x, y):
    global E_0, Gamma
    E_0, Gamma = x, y
    return -2*np.sum(np.log(f_both(E)))

def get_contour_verts(cn):
    '''
    copied from max_like_errors.py in Lecture file.
    '''
    contours = []
    # for each contour line
    for cc in cn.collections:
        paths = []
        # for each separate section of the contour line
        for pp in cc.get_paths():
            xy = []
            # for each segment of that section
            for vv in pp.iter_segments():
                xy.append(vv[0])
            paths.append(np.vstack(xy))
        contours.append(paths)
    return contours

E_0_array = np.linspace(198, 215, 100)
Gamma_array = np.linspace(16, 32, 100)
E_0_grid, Gamma_grid = np.meshgrid(E_0_array, Gamma_array)

v_logL = np.vectorize(logL)
logL_grid = v_logL(E_0_grid, Gamma_grid)

fig, ax = plt.subplots(figsize = (10, 6))

contourf = ax.contourf(E_0_grid, Gamma_grid, logL_grid, levels = np.linspace(logL_min, logL_min+17, 100))
contour = ax.contour(E_0_grid, Gamma_grid, logL_grid, levels = [logL_min+1], colors='white')
ax.scatter(E_0_fit, Gamma_fit, label = "best fit of $E_0\ \mathrm{and}\ \Gamma$", marker = "+", c = "red", s = 50)
##########################################################
# draw the circumscribed rectangle tangent to the contour.
contour_vertices = get_contour_verts(contour)
# find the max and min of the contour
E_0_min = np.min(contour_vertices[0][0][:,0])
E_0_max = np.max(contour_vertices[0][0][:,0])
Gamma_min = np.min(contour_vertices[0][0][:,1])
Gamma_max = np.max(contour_vertices[0][0][:,1])
print('The scan of the -2log(L) map results in:')
print("E_0 = {:.3f} +{:.3f}/-{:.3f} a.u.".format(E_0_fit, E_0_max-E_0_fit, E_0_fit-E_0_min))
print("Gamma = {:.3f} +{:.3f}/-{:.3f} a.u.".format(Gamma_fit, Gamma_max-Gamma_fit, Gamma_fit-Gamma_min))
print('The errors from a -2log(L) scan do not have to be symmetric')

ax.vlines([E_0_min, E_0_max], ymin = Gamma_array.min(), ymax = Gamma_array.max(),linestyles = 'dashed', colors='white', linewidth = 0.5)
ax.hlines([Gamma_min, Gamma_max], xmin = E_0_array.min(), xmax = E_0_array.max(), linestyles = 'dashed', colors = 'white', linewidth = 0.5)

ax.set_title(r"$-2\log(L)$ as a function of $E_0$ and $\Gamma$ for the best fit value of $n_s$")
ax.set_xlabel("$E_0\ [\mathrm{a.u.}]$")
ax.set_ylabel("$\Gamma\ [\mathrm{a.u.}]$")
fig.colorbar(contourf, label = r"$-2\log L$")
ax.clabel(contour, fontsize=12, colors = ('w'), fmt = (r"$-2logL_{max} + 1$"))
plt.legend()
plt.show()

"""## 7."""

print("Gamma is the half-width at half-maximum and is sometimes called the probable error.")

"""# 2. Chi-squared fitting.

## 1.
"""

fig, ax = plt.subplots(figsize = (8,6))
counts, bins, patches = ax.hist(E, histtype='step', bins = 25, range = (0, 400), label = 'photon energy data')
# print(counts, bins)
ax.legend()
ax.set_xlabel(r"$E\ [\rm a.u.]$")
ax.set_ylabel(r"$\rm counts\ [a.u.^{-1}]$")
ax.set_title("the photon energy data in 25 equally-spaced bins")
plt.show()

"""## 2."""

from scipy.optimize import curve_fit
from scipy.stats import chi2

E_b = 100
x = (bins[0:-1]+bins[1:])/2
y = counts
# print('x',x)
delta_x = (bins[-1]-bins[0]) / x.size
# print('delta', delta_x)

def f_s(E, n_s, E_0, Gamma):
    return 1/(np.pi * Gamma * (1 + ((E - E_0)/Gamma)**2))

def f_B(E, n_s, E_0, Gamma):
    return np.e**(-E/E_b)/E_b

def line(E, n_s, E_0, Gamma):
    return n_s/N * f_s(E, n_s, E_0, Gamma) + (N - n_s)/N * f_B(E, n_s, E_0, Gamma)

def chi2_for_fit(arguments):
    # global n_s, E_0, Gamma
    n_s, E_0, Gamma = arguments
    mu = line(x, n_s, E_0, Gamma)*delta_x*N
    # print("y = {}, mu ={}".format(y, mu))
    return np.sum((y - mu)**2/mu)

# initial_guess = [N/2, 200, 100]
# bounds = ((0,N), (0, 500), (1, 200))
min_outcome = minimize(chi2_for_fit, x0 = initial_guess, method = 'BFGS')#, bounds = bounds)

print("This minimization is success:", min_outcome.success)
n_s_fit = min_outcome.x[0]
E_0_fit = min_outcome.x[1]
Gamma_fit = min_outcome.x[2]

n_s_unc = np.sqrt(min_outcome.hess_inv[0,0])
E_0_unc = np.sqrt(min_outcome.hess_inv[1,1])
Gamma_unc = np.sqrt(min_outcome.hess_inv[2,2])


# popt, pcov = curve_fit(line,x,y,sigma=[1]*x.size,p0=initial_guess)

# n_s_fit = popt[0]
# E_0_fit = popt[1]
# Gamma_fit = popt[2]

# n_s_unc = np.sqrt(pcov[0,0])
# E_0_unc = np.sqrt(pcov[1,1])
# Gamma_unc = np.sqrt(pcov[2,2])

# print("Minimize Chi-square")
print("Best fit values are:")
print(format("n_s", "^10"), "= {:.0f} \u00B1 {:.0f}".format(n_s_fit,n_s_unc))
print(format("n_b", "^10"), "= {:.0f} \u00B1 {:.0f}".format(N - n_s_fit,n_s_unc))
print(format("E_0", "^10"), "= {:.3f} \u00B1 {:.3f} a.u.".format(E_0_fit,E_0_unc))
print(format("Gamma", "^10"), "= {:.3f} \u00B1 {:.3f} a.u.".format(Gamma_fit,Gamma_unc))

dof = np.size(x)-3 # remove 3 for fitting n_s, E_0 and Gamma
print("There are {:d} degrees of freedom.".format(dof))

"""## 3."""

# def chisquared(x,y,a,b,c):
#     mu = line(x, a, b, c)*delta_x
#     return np.sum((y - mu)**2/mu)

# chi2min = chisquared(x,y,n_s_fit,E_0_fit,Gamma_fit)

chi2min = min_outcome.fun
# print(min_outcome)
print("\nThe minimized chi-Squared is {:.3f}".format(chi2min))
print("The p-value for this fit is {:.3f}".format(1-chi2.cdf(chi2min,dof)))

"""## 4.

Compared with result in part 1.4 and 1.6, we can see that the results from *Maximum likelihood* and *Chi-squared fitting* are pretty close. Hence *chi-square* for binned data is a good method to estimate the parameters in this problem.

If the data follow the assumed Gaussian distributions, the $p$-value is expected to be a random variable uniformly distributed from 0 to 1. Obtaining a small $p$-value of the fit could indicate that the theoretical model $y = f(x;\vec \theta)$ is not good. For this reason, the minimum $\chi^2$ value can be used as a measurement of the goodness of the fit and $\chi^2_{min} \approx 17$ is quite reasonable. Anyway, we expect to obtain a $p$-value more than a threshold (0.05 for example), which is compatible with the results in part 2.3. Hence, I trust the p-value calculated on part 2.3.
"""

