# -*- coding: utf-8 -*-
"""BaggingTrees_moons.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cYwECBrPZt3l9FFZDrlA-in5u2iuDsd5
"""

from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import numpy as np

# Create moons
# It's worth trying this with noise=0 (default)
# The noise=0.3 is a convolution of the default with a normal of std=0.3
# Create 10,000 samples of each moon

# Note that writing the likelihood ratio test for this dataset is NOT hard.
# But I won't do that here.

X,z = make_moons(noise=0.3,n_samples=10000,random_state=1)

# remap colors - see previous scripts
cm_bright = ListedColormap(["#FF8800", "#0000FF"]) # First color is orange, second is blue
plt.scatter(X.T[0],X.T[1],s=0.5,c=z,cmap=cm_bright)

# Train the decision tree
# Play with
#      n_estimators = 500,1000,2000
#      max_samples = 100,250,500 -> size of training subset

# I optimized these settings separately
# Default n_estimators=1000 max_samples=250
# n_jobs=-1 : Use all available CPU cores

# As written, this will choose the defaults of DecisionTreeClassifier()
# The default on DecisionTreeClassifer is no max_depth and min_samples_leaf=2
# These trees will often be overtrained. But voting will fix this.


X_train,X_test,z_train,z_test = train_test_split(X,z,test_size=0.2,random_state=1)
bag_clf = BaggingClassifier(DecisionTreeClassifier(),
                            n_estimators=1000,
                            max_samples=250,
                            bootstrap=True,
                            n_jobs=-1)
bag_clf.fit(X_train,z_train)

z_predict = bag_clf.predict(X_test)
accuracy_score(z_test,z_predict)

def plot_decision_boundary(clf,x,z):
    x_min=np.min(x[:,0])
    x_max=np.max(x[:,0])
    y_min=np.min(x[:,1])
    y_max=np.max(x[:,1])
    step=0.02
    xx,yy = np.meshgrid(np.arange(x_min,x_max,step),np.arange(y_min,y_max,step))

    grid = np.vstack([xx.ravel(), yy.ravel()]).T
    Z = clf.predict(grid)
    Z = np.reshape(Z,xx.shape)

    plt.contour(xx,yy,Z,colors='k',linewidths=1)
    plt.scatter(x[:,0],x[:,1],s=0.5,c=z,cmap=cm_bright)


plot_decision_boundary(bag_clf,X,z)

feature_importances = np.mean([tree.feature_importances_ for tree in bag_clf.estimators_], axis=0)

print(feature_importances)

